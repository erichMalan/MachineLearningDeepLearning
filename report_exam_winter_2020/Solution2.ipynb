{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡] 100%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.naive_bayes import ComplementNB\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from nltk.tokenize import sent_tokenize\n",
    "#from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.svm import SVC\n",
    "import it_core_news_sm\n",
    "nlp = it_core_news_sm.load(disable=['tagger','textcat','ner','parser'])\n",
    "import string\n",
    "import re\n",
    "import progressbar\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from unicodedata import name\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.pipeline import make_pipeline as mp\n",
    "from sklearn.pipeline import FeatureUnion as fu\n",
    "\n",
    "%matplotlib inline\n",
    "url = 'https://raw.githubusercontent.com/mrblasco/genderNamesITA/master/gender_firstnames_ITA.csv'\n",
    "itanames = pd.read_csv(url, error_bad_lines=False)['nome']\n",
    "itanames = itanames[~itanames.str.isalpha()==False]                                                                  \n",
    "mysw = sw.words(\"italian\")\n",
    "mysw = itanames.values.tolist() +['milano','venezia']\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self,total_docs=0,stop_words=[],common_words=[]):\n",
    "        self.lemmatizer = nlp\n",
    "        self.stemmer = SnowballStemmer(\"italian\")\n",
    "        self.total_docs = total_docs\n",
    "        self.num_doc = 0\n",
    "        self.bar = None\n",
    "        stop_words = \" \".join(stop_words)\n",
    "        stop_words = ''.join(c[0] for c in itertools.groupby(stop_words))\n",
    "        self.stop_words = [\"esser\",\"eser\",\"essere\",\"esere\",\"il\"]\n",
    "\n",
    "        for sw in nlp(stop_words):\n",
    "            self.stop_words.append(self.stemmer.stem(sw.lemma_))\n",
    "        self.common_words = common_words\n",
    "    \n",
    "    def __call__(self, document):\n",
    " \n",
    "        if(self.num_doc == 0):\n",
    "            self.bar = progressbar.ProgressBar(maxval=self.total_docs, \\\n",
    "                   widgets=[progressbar.Bar('≡', '[', ']'), ' ', progressbar.Percentage()])\n",
    "            self.bar.start()\n",
    "        self.num_doc += 1\n",
    "        document = ''.join(c[0] for c in itertools.groupby(document))\n",
    "\n",
    "        document = re.sub('[^A-Za-zéèòçàù\\s]+', ' ', document)\n",
    "        document = re.sub('k', 'ch', document)\n",
    "        document = re.sub('wi fi', 'wifi', document)\n",
    "        document = re.sub('isim', '', document)\n",
    "        document = re.sub('albergo', 'hotel', document)\n",
    "        document = re.sub('hotel', 'strutur', document)\n",
    "        document = re.sub('cordiale', 'gentile', document)\n",
    "\n",
    "        \n",
    "        lemmas = []\n",
    "        for tt in self.lemmatizer(document):\n",
    "            if tt.text.isalpha():\n",
    "                t = tt.lemma_.strip()\n",
    "                if(t == \"no\" or t == \"non\" or t == \"not\"):\n",
    "                    lemmas.append(\"no\")\n",
    "                    continue\n",
    "                if t == \"stella\":\n",
    "                    lemmas.append(t)\n",
    "                    continue\n",
    "                t = self.stemmer.stem(t)\n",
    "                if len(t) >= 2 and not tt.is_stop and t not in self.stop_words or t.startswith('molt'):\n",
    "                    lemmas.append(t)\n",
    "        if(self.num_doc >= self.total_docs):\n",
    "            self.bar.finish()\n",
    "        else:\n",
    "            self.bar.update(self.num_doc)   \n",
    "        return lemmas\n",
    "\n",
    "\n",
    "    def clear_bar(self,total_docs=0):\n",
    "        self.num_doc = 0\n",
    "        self.total_docs = total_docs\n",
    "    \n",
    "datadir = \"./datasrc/dataset_winter_2020/\"\n",
    "\n",
    "#dataset used in program\n",
    "datadev = pd.read_csv(datadir+\"development.csv\").to_numpy()\n",
    "dataeva = pd.read_csv(datadir+\"evaluation.csv\")\n",
    "\n",
    "tokenizer = LemmaTokenizer(total_docs=datadev[:,0].size,stop_words=mysw)\n",
    "vectorizer = TfidfVectorizer(input='content',tokenizer=tokenizer,ngram_range = (1,3),max_df=0.9,min_df=0.0003,encoding=\"utf-8\")\n",
    "X_tfidf = vectorizer.fit_transform(datadev[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=70, random_state=42, algorithm = 'arpack',tol=0)\n",
    "X_svd = svd.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_weighted for each iteration:[0.94714691 0.94915492 0.94016083 0.94397755 0.94367385]\n",
      "f1_weighted (statistics): 0.945 (+/- 0.006)\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "model = SVC()\n",
    "score_type = \"f1_weighted\"\n",
    "cvs = cross_val_score(model,X_svd,datadev[:,1],cv=5,scoring = score_type, n_jobs = 7)\n",
    "print(f\"{score_type} for each iteration:{cvs}\")\n",
    "print(f\"{score_type} (statistics): {cvs.mean():.3f} (+/- {cvs.std() * 2:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡] 100%\n"
     ]
    }
   ],
   "source": [
    "model = SVC(kernel=\"rbf\")\n",
    "tokenizer.clear_bar(total_docs=dataeva['text'].count())\n",
    "x_test = vectorizer.transform(dataeva['text'])\n",
    "x_test_svd = svd.transform(x_test)\n",
    "model.fit(X_svd,datadev[:,1])\n",
    "y_pred = model.predict(x_test_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_solution(y_pred):\n",
    "    fff1 = np.asarray([[\"Id\",\"Predicted\"]])\n",
    "    fff2 = np.column_stack((dataeva.index.values,y_pred))\n",
    "    fff3 = np.concatenate((fff1,fff2))\n",
    "    np.savetxt(datadir+\"sample_submission.csv\", fff3,fmt='%s', delimiter=\",\")\n",
    "print_solution(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.manifold import TSNE\n",
    "#from sklearn.pipeline import make_pipeline as mp\n",
    "#tsne = TSNE(n_components=50, perplexity=50, verbose=1, init='random', learning_rate=2000,n_iter=1000,early_exaggeration=12, method='exact')\n",
    "#X_svd = svd.fit_transform(X_tfidf)\n",
    "#pipeline = mp(svd, tsne)\n",
    "#red_X = pipeline.fit_transform(X_tfidf)\n",
    "\n",
    "#tsne = TSNE(n_components=50, perplexity=50, verbose=1, init='random', learning_rate=2000,n_iter=1000,early_exaggeration=12, method='exact')\n",
    "#X_svd = svd.fit_transform(X_tfidf)\n",
    "#pipeline = make_pipeline(svd, tsne)\n",
    "#red_X = pipeline.fit_transform(X_tfidf)\n",
    "\n",
    "#model = LogisticRegression(n_jobs=6) #used just for fast testing lemmatizer and vectorizer -> svc too slow but higher accuracy\n",
    "model = SVC(kernel=\"rbf\",C=2.4,gamma=0.72,class_weight='balanced')\n",
    "score_type = \"f1_weighted\"\n",
    "cvs = cross_val_score(model,X_svd,datadev['class'],cv=5,scoring = score_type, n_jobs = 7)\n",
    "print(f\"{score_type} for each iteration:{cvs}\")\n",
    "print(f\"{score_type} (statistics): {cvs.mean():.3f} (+/- {cvs.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.text import FreqDistVisualizer\n",
    "features   = vectorizer.get_feature_names()\n",
    "\n",
    "visualizer = FreqDistVisualizer(features=features, orient='v')\n",
    "visualizer.fit(X_tfidf)\n",
    "visualizer.show()\n",
    "\n",
    "visualizer = FreqDistVisualizer(features=features, orient='v')\n",
    "visualizer.fit(X_tfidf[datadev[:,1]=='pos'])\n",
    "visualizer.show()\n",
    "\n",
    "visualizer = FreqDistVisualizer(features=features, orient='v')\n",
    "visualizer.fit(X_tfidf[datadev[:,1]=='neg'])\n",
    "visualizer.show()\n",
    "\n",
    "\n",
    "from yellowbrick.text import TSNEVisualizer\n",
    "tsne = TSNEVisualizer()\n",
    "tsne.fit(X_svd,datadev[:,1],alphafloat = 0.01,decomposestring=None)\n",
    "tsne.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE()\n",
    "red_X = tsne.fit_transform(X_svd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.sparse import hstack\n",
    "\n",
    "X_pos = X_tfidf[datadev[:,1]=='pos']\n",
    "X_neg = X_tfidf[datadev[:,1]=='neg']\n",
    "\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "pos_svd = svd.fit_transform(X_pos)\n",
    "neg_svd = svd.transform(X_neg)\n",
    "\n",
    "combined_svd = np.concatenate((pos_svd, neg_svd), axis=0)\n",
    "\n",
    "print(combined_svd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_pos = datadev[datadev[:,1]=='pos'][:,0]\n",
    "X_neg = datadev[datadev[:,1]=='neg'][:,0]\n",
    "pos_docs = X_pos.shape\n",
    "neg_docs = X_neg.shape\n",
    "total_docs = pos_docs + neg_docs\n",
    "evadocs = []\n",
    "\n",
    "num_words = [] \n",
    "num_p_words = []\n",
    "num_n_words = []\n",
    "\n",
    "for doc in X_pos:\n",
    "    size = len(doc)\n",
    "    num_p_words.append(size)\n",
    "    \n",
    "for doc in X_neg:\n",
    "    size = len(doc)\n",
    "    num_n_words.append(size)\n",
    "    \n",
    "num_words = num_p_words + num_n_words\n",
    "\n",
    "for doc in dataeva[:,0]:\n",
    "    evadocs.append(len(doc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_elements(seq) -> dict:\n",
    "    hist = {}\n",
    "    for i in seq:\n",
    "        hist[i] = hist.get(i, 0) + 1\n",
    "    hist = {np.log(k): np.log(v) for k, v in hist.items()}\n",
    "    hist = np.array(list(hist.items()))\n",
    "    return hist\n",
    "\n",
    "pos_dict = count_elements(num_p_words)\n",
    "neg_dict = count_elements(num_n_words)\n",
    "all_dict = count_elements(num_words)\n",
    "eva_dict = count_elements(evadocs)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_histogram(p = {}, n = {}, a = {},eva = {}):\n",
    "    kwargs = dict(histtype='stepfilled', alpha=1,bins=30)\n",
    "    #n, bins, patches = plt.hist(x=dic.keys(), bins='auto', color='#0504aa',\n",
    "    #                            alpha=0.7, rwidth=0.85)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.xlabel('Length (log)')\n",
    "    plt.ylabel('Documents (log)')\n",
    "    plt.title('development')\n",
    "    #plt.text(23, 45, r'$\\mu=15, b=3$')\n",
    "    #maxfreq = 10000\n",
    "    # Set a clean upper y-axis limit.\n",
    "    #plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)\n",
    "    plt.hist(a[:,0], **kwargs,label='all',weights=a[:,1])\n",
    "    plt.hist(p[:,0], **kwargs,label='pos',weights=p[:,1])\n",
    "    plt.hist(n[:,0], **kwargs,label='neg',weights=n[:,1])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    plt.xlabel('Length (log)')\n",
    "    plt.ylabel('Documents (log)')\n",
    "    plt.title('evaluation')\n",
    "    plt.hist(eva[:,0], **kwargs,label='eva',weights=eva[:,1])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    #plt.hist2d(pt.values(), p.values(), bins=30, cmap='Blues')\n",
    "    #cb = plt.colorbar()\n",
    "    #cb.set_label('counts in bin')\n",
    "    #plt.show()\n",
    "    #plt.hist2d(nt.values(), n.values(), bins=30, cmap='Greens')\n",
    "    #cb = plt.colorbar()\n",
    "    #cb.set_label('counts in bin')\n",
    "    #plt.show()\n",
    "    #plt.hist2d(at.values(), a.values(), bins=30, cmap='Reds')\n",
    "    #cb = plt.colorbar()\n",
    "    #cb.set_label('counts in bin')\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "plot_histogram(pos_dict,neg_dict,all_dict,eva_dict)\n",
    "print(\"development statistics:\")\n",
    "print(max(num_words),min(num_words),sum(num_words)/len(num_words))\n",
    "print(\"development statistics: pos\")\n",
    "print(max(num_p_words),min(num_p_words),sum(num_p_words)/len(num_p_words))\n",
    "print(\"development statistics: neg\")\n",
    "print(max(num_n_words),min(num_n_words),sum(num_n_words)/len(num_n_words))\n",
    "print(\"evaluation statistics:\")\n",
    "print(max(evadocs),min(evadocs),sum(evadocs)/len(evadocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=5500, random_state=42)\n",
    "print(f\"Total variance explained: {np.sum(svd.explained_variance_ratio_):.2f}\")\n",
    "cum_variance = np.cumsum(svd.explained_variance_ratio_)\n",
    "idx = np.argmax(cum_variance > .85)\n",
    "print(idx)\n",
    "tsne = TSNE(n_components=50, perplexity=50, verbose=1, init='random', learning_rate=2000,n_iter=1000,early_exaggeration=12, method='exact')\n",
    "#X_svd = svd.fit_transform(X_tfidf)\n",
    "pipeline = make_pipeline(svd, tsne)\n",
    "red_X = pipeline.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_positions = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "tfidf = X_tfidf[datadev[:,1]=='pos']\n",
    "tfidf_sum = np.sum(tfidf, axis=0) # numpy.matrix\n",
    "tfidf_sum = np.asarray(tfidf_sum).reshape(-1) # numpy.array of shape (1, X.shape[1])\n",
    "top_indices = tfidf_sum.argsort()\n",
    "\n",
    "top_indices = top_indices[-50:]\n",
    "p = {word_positions[idx]: tfidf_sum[idx] for idx in top_indices}.keys()\n",
    "#print(p)\n",
    "tfidf = X_tfidf[datadev[:,1]=='neg']\n",
    "tfidf_sum = np.sum(tfidf, axis=0) # numpy.matrix\n",
    "tfidf_sum = np.asarray(tfidf_sum).reshape(-1) # numpy.array of shape (1, X.shape[1])\n",
    "top_indices = tfidf_sum.argsort()\n",
    "\n",
    "top_indices = top_indices[-50:]\n",
    "n = {word_positions[idx]: tfidf_sum[idx] for idx in top_indices}.keys()\n",
    "#print(n)\n",
    "\n",
    "#print(n.keys() - (p.keys() & n.keys()))\n",
    "\n",
    "word_positions = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "tfidf = X_tfidf\n",
    "tfidf_sum = np.sum(tfidf, axis=0) # numpy.matrix\n",
    "tfidf_sum = np.asarray(tfidf_sum).reshape(-1) # numpy.array of shape (1, X.shape[1])\n",
    "top_indices = tfidf_sum.argsort()\n",
    "top_indices = top_indices[-50:]\n",
    "a = {word_positions[idx]: tfidf_sum[idx] for idx in top_indices}\n",
    "a = a.keys() & set(list(n)+list(p))\n",
    "common = p & n\n",
    "print(p-common)\n",
    "print(n - common)\n",
    "print(common)\n",
    "#print(a-common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"uppercase\" in vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_positions = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "labels = ['pos','neg']\n",
    "_words = []\n",
    "for label in labels:\n",
    "\n",
    "    # compute the total tfidf for each label\n",
    "    tfidf = X_tfidf[datadev['class'].to_numpy() == label]\n",
    "    tfidf_sum = np.sum(tfidf, axis=0) # numpy.matrix\n",
    "    tfidf_sum = np.asarray(tfidf_sum).reshape(-1) # numpy.array of shape (1, X.shape[1])\n",
    "    top_indices = tfidf_sum.argsort()\n",
    "\n",
    "    top_indices = top_indices[-10:]\n",
    "    _words.append({word_positions[idx]: tfidf_sum[idx] for idx in top_indices})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_polarity = {}\n",
    "for pword in _words[0].keys():\n",
    "    for nword in _words[1].keys():\n",
    "        if(pword == nword):\n",
    "            common_words_polarity[pword] = _words[0][pword]-_words[1][pword]\n",
    "\n",
    "only_pos = set(_words[1].keys()) - set(set(_words[0].keys()) & set(_words[1].keys()))\n",
    "only_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos = datadev[datadev['class']=='pos']\n",
    "X_neg = datadev[datadev['class']=='neg']\n",
    "Xp_tfidf = vectorizer.fit_transform(X_pos)\n",
    "Xn_tfidf = vectorizer.fit_transform(X_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(datadev['text'].to_numpy(), datadev['class'].to_numpy(), test_size=0.33, random_state=0)\n",
    "\n",
    "#train preprocessing\n",
    "tokenizer2 = LemmaTokenizer(total_docs = datadev['text'].count(),stop_words=mysw)\n",
    "vectorizer2 = TfidfVectorizer(input='content',tokenizer=tokenizer,ngram_range = (1,3),max_df=0.9,min_df=0.0003,encoding=\"utf-8\",use_idf=False)#use_idf=False)#,ngram_range=(3,4))#,max_df=0.62)#,strip_accents='unicode',max_df=1.0)#,min_df=0.01)\n",
    "X_train_tfidf = vectorizer2.fit_transform(X_train)\n",
    "print(\"vectorization train done...\")\n",
    "\n",
    "#test preprocessing\n",
    "\n",
    "X_test_tfidf = vectorizer2.transform(X_test)\n",
    "#X_test_svd = svd.fit_transform(X_test_tfidf)\n",
    "print(\"vectorization test done...\")\n",
    "\n",
    "testclsf = SVC(kernel=\"rbf\",C=2,gamma=0.73,class_weight='balanced')\n",
    "testclsf.fit(X_train_tfidf, y_train)\n",
    "y_pred_test = testclsf.predict(X_test_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_pred_wrong_tfidf = X_test_tfidf[y_test != y_pred_test]\n",
    "def generate_wordclouds(X_tfidf, y_lab ,word_positions,title = \"\"):\n",
    "    \n",
    "    labels = ['pos','neg']\n",
    "    top_count = 8\n",
    "    min_support = 0.3\n",
    "    dist_words = sorted(v for k, v in word_positions.items())\n",
    "    _words = []\n",
    "\n",
    "    for label in labels:\n",
    "        \n",
    "        # compute the total tfidf for each label\n",
    "        tfidf = X_tfidf[y_lab == label]\n",
    "        tfidf_sum = np.sum(tfidf, axis=0) # numpy.matrix\n",
    "        tfidf_sum = np.asarray(tfidf_sum).reshape(-1) # numpy.array of shape (1, X.shape[1])\n",
    "        top_indices = tfidf_sum.argsort()\n",
    "        \n",
    "        top_indices = top_indices[-top_count:]\n",
    "        _words.append({word_positions[idx]: tfidf_sum[idx] for idx in top_indices})\n",
    "        \n",
    "        term_weights = {word_positions[idx]: tfidf_sum[idx] for idx in top_indices}\n",
    "        wc = WordCloud(width=1200, height=800, background_color=\"white\")\n",
    "        wordcloud = wc.generate_from_frequencies(term_weights)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "        ax.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax.axis(\"off\")\n",
    "        fig.suptitle(f\"{title} sentiment {label}\") \n",
    "    \n",
    "    return _words\n",
    "\n",
    "\n",
    "word_positions = {v: k for k, v in vectorizer2.vocabulary_.items()}\n",
    "shouldbe = generate_wordclouds(X_tfidf, y_test[y_test != y_pred_test],word_positions)\n",
    "reallyis = generate_wordclouds(X_tfidf, y_pred_test[y_test != y_pred_test],word_positions)\n",
    "\n",
    "for i in range(2):\n",
    "    common = set(set(shouldbe[i]) & set(reallyis[i]))\n",
    "    print(str(i)+\"-shouldbe:\")\n",
    "    print(set(shouldbe[i])-common)\n",
    "    print(str(i)+\"-reallyis:\")\n",
    "    print(set(reallyis[i])-common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot((datadev=='pos'),rapporto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
