{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.svm import SVC\n",
    "import it_core_news_sm\n",
    "nlp = it_core_news_sm.load(disable=['tagger','textcat','ner','parser'])\n",
    "import string\n",
    "import re\n",
    "import progressbar\n",
    "import matplotlib.pyplot as plt\n",
    "from unicodedata import name\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mysw = sw.words(\"italian\")\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self,total_docs=0,stop_words=[],common_words=[]):\n",
    "        self.lemmatizer = nlp\n",
    "        self.stemmer = SnowballStemmer(\"italian\")\n",
    "        self.total_docs = total_docs\n",
    "        self.num_doc = 0\n",
    "        self.bar = None\n",
    "        stop_words = \" \".join(stop_words)\n",
    "        stop_words = ''.join(c[0] for c in itertools.groupby(stop_words))\n",
    "        self.stop_words = [\"esser\",\"eser\",\"essere\",\"esere\"]\n",
    "\n",
    "        for sw in nlp(stop_words):\n",
    "            self.stop_words.append(self.stemmer.stem(sw.lemma_))\n",
    "        self.common_words = common_words\n",
    "    \n",
    "    \n",
    "    def __call__(self, document):\n",
    " \n",
    "        if(self.num_doc == 0):\n",
    "            self.bar = progressbar.ProgressBar(maxval=self.total_docs, \\\n",
    "                   widgets=[progressbar.Bar('≡', '[', ']'), ' ', progressbar.Percentage()])\n",
    "            self.bar.start()\n",
    "        self.num_doc += 1\n",
    "        document = ''.join(c[0] for c in itertools.groupby(document))\n",
    "        try:\n",
    "            document = re.sub('[\\U0001F602-\\U0001F64F]', lambda m: \" \"+name(m.group())+ \" \", document)\n",
    "        except:\n",
    "            i=0\n",
    "        try:\n",
    "            document = re.sub('?', ' interogative ', document)\n",
    "        except:\n",
    "            i=0\n",
    "        try:\n",
    "            document = re.sub('!', ' esclamative ', document)\n",
    "        except:\n",
    "            i=0\n",
    "        document = re.sub('[^A-Za-zéèòçàù\\s]+', ' ', document)\n",
    "        document = re.sub('k', 'ch', document)\n",
    "        document = re.sub('wi fi', 'wifi', document)\n",
    "        \n",
    "        lemmas = []\n",
    "        for tt in self.lemmatizer(document):\n",
    "            if tt.text.isalpha():\n",
    "                t = tt.lemma_.strip()\n",
    "                t = self.stemmer.stem(t)\n",
    "                if(t == \"no\" or t == \"non\" or t == \"not\"):\n",
    "                    lemmas.append(\"no\")\n",
    "                elif(t.startswith('molt') or t.startswith('stel')):\n",
    "                    lemmas.append(t)\n",
    "                elif len(t) >= 2 and len(t)<16 and t not in self.stop_words:\n",
    "                    t = self.stemmer.stem(t)\n",
    "                    lemmas.append(t)\n",
    "        if(self.num_doc >= self.total_docs):\n",
    "            self.bar.finish()\n",
    "        else:\n",
    "            self.bar.update(self.num_doc)   \n",
    "        return lemmas\n",
    "\n",
    "    \n",
    "    def __call2__(self,document):\n",
    "        #print(\"\\n\\n\\n\\n\\n\\n\"+document)\n",
    "        if(self.num_doc == 0):\n",
    "            self.bar = progressbar.ProgressBar(maxval=self.total_docs, \\\n",
    "                   widgets=[progressbar.Bar('≡', '[', ']'), ' ', progressbar.Percentage()])\n",
    "            self.bar.start()\n",
    "        self.num_doc += 1\n",
    "        lemmas = []\n",
    "        document = ''.join(c[0] for c in itertools.groupby(document))\n",
    "        document = re.sub('[\\,\\;\\.\\:\\-\\(\\)\\\"\\!\\?]+', ' | ', document)\n",
    "        document = re.sub('[^A-Za-z0-9éèòçàù|]+', ' ', document)\n",
    "        for subdoc in sent_tokenize(document):\n",
    "            ds = document.split(\"|\")\n",
    "            for sents in ds:\n",
    "                current_sents = []\n",
    "                for t in nlp(sents): \n",
    "                    pt = self.stemmer.stem(t.lemma_)\n",
    "                    if(t.lemma_ == \"no\" or t.lemma_ == \"non\"):\n",
    "                        #lemmas.append(pt)\n",
    "                        current_sents.append(pt)\n",
    "                    elif(not t.is_alpha or len(pt) < 2 or len(pt) > 16 or pt in self.stop_words):\n",
    "                        continue\n",
    "                    elif(t.is_stop or pt in self.common_words):\n",
    "                        current_sents.append(pt)\n",
    "                    else:\n",
    "                        lemmas.append(pt)\n",
    "                        current_sents.append(pt)\n",
    "                \n",
    "                ran = len(current_sents)-3\n",
    "                if(ran>-1):\n",
    "                    current_sents.sort()\n",
    "                    for s in range(ran): #ngram_range optimization\n",
    "                        lemmas.append(current_sents[s]+\" \"+current_sents[s+1])\n",
    "                        lemmas.append(current_sents[s]+\" \"+current_sents[s+1] + \" \" + current_sents[s+2])\n",
    "                    lemmas.append(current_sents[-1]+\" \"+current_sents[-2])\n",
    "                elif(ran > -2):\n",
    "                    current_sents.sort()\n",
    "                    lemmas.append(current_sents[0]+\" \"+current_sents[1])\n",
    "                            \n",
    "        if(self.num_doc >= self.total_docs):\n",
    "            self.bar.finish()\n",
    "        else:\n",
    "            self.bar.update(self.num_doc)\n",
    "        return lemmas\n",
    "    \n",
    "    def clear_bar(self,total_docs=0):\n",
    "        self.num_doc = 0\n",
    "        self.total_docs = total_docs\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡] 100%\n"
     ]
    }
   ],
   "source": [
    "datadir = \"./datasrc/dataset_winter_2020/\"\n",
    "\n",
    "#dataset used in program\n",
    "datadev = pd.read_csv(datadir+\"development.csv\")\n",
    "dataeva = pd.read_csv(datadir+\"evaluation.csv\")\n",
    "\n",
    "tokenizer = LemmaTokenizer(total_docs=datadev['text'].count(),stop_words=mysw)\n",
    "vectorizer = TfidfVectorizer(input='content',encoding=\"utf-8\",tokenizer=tokenizer,ngram_range = (1,3),max_df=0.90,min_df=0.0003)#, use_idf=False)#,ngram_range=(3,4))#,max_df=0.62)#,strip_accents='unicode',max_df=1.0)#,min_df=0.01)\n",
    "X_tfidf = vectorizer.fit_transform(datadev['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡] 100%\n"
     ]
    }
   ],
   "source": [
    "model = SVC(kernel=\"rbf\", C=2.2 , gamma=0.725 , class_weight='balanced')\n",
    "tokenizer.clear_bar(total_docs=dataeva['text'].count())\n",
    "x_test = vectorizer.transform(dataeva['text'])\n",
    "model.fit(X_tfidf,datadev['class'].to_numpy())\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_solution(y_pred):\n",
    "    fff1 = np.asarray([[\"Id\",\"Predicted\"]])\n",
    "    fff2 = np.column_stack((dataeva.index.values,y_pred))\n",
    "    fff3 = np.concatenate((fff1,fff2))\n",
    "    np.savetxt(datadir+\"sample_submission.csv\", fff3,fmt='%s', delimiter=\",\")\n",
    "print_solution(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_weighted for each iteration:[0.96074725 0.95941106 0.9538043  0.95639121 0.95980215]\n",
      "f1_weighted (statistics): 0.958 (+/- 0.005)\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB() \n",
    "score_type = \"f1_weighted\"\n",
    "cvs = cross_val_score(model,X_tfidf,datadev['class'],cv=5,scoring = score_type, n_jobs = 7)\n",
    "print(f\"{score_type} for each iteration:{cvs}\")\n",
    "print(f\"{score_type} (statistics): {cvs.mean():.3f} (+/- {cvs.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_weighted for each iteration:[0.96913932 0.97098886 0.9642998  0.96747226 0.96614143]\n",
      "f1_weighted (statistics): 0.968 (+/- 0.005)\n"
     ]
    }
   ],
   "source": [
    "#model = MultinomialNB() #used just for fast testing lemmatizer and vectorizer -> svc too slow\n",
    "model = SVC(kernel=\"rbf\", C=2.2 , gamma=0.725 , class_weight='balanced')\n",
    "score_type = \"f1_weighted\"\n",
    "cvs = cross_val_score(model,X_tfidf,datadev['class'],cv=5,scoring = score_type, n_jobs = 7)\n",
    "print(f\"{score_type} for each iteration:{cvs}\")\n",
    "print(f\"{score_type} (statistics): {cvs.mean():.3f} (+/- {cvs.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2.2, 'gamma': 0.725}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C10   gamma 1     - Cs = [0.01,0.1,1,10]                         gammas = [0.001,0.01,0.1,1]\n",
    "# C8    gamma 0.8   - Cs = [8,9,10,11,12]                          gammas = [0.6,0.8,1,1.2,1.4]\n",
    "# C5    gamma 0.7   - Cs = [5,6,7,8]                               gammas = [0.7,0,75,0.8,0.85,0.9]\n",
    "# C2    gamma 0.73  - Cs = [2,3,4,5,5.5]                           gammas = [0.65,0.67,0.69,0.71,0.73]\n",
    "# C2.4  gamma 0.72  - Cs = [1.4,1.6,1.8,2,2.2,2.4]                 gammas = [0.72,0.73,0.74]\n",
    "# C2.2  gamma 0.725 - Cs = [2.1,2.2,2.3,2.4,2.5,2.6,2.7,2.8,2.9]   gammas = [0.725,0.73,0.735]\n",
    "\n",
    "Cs = [2.15,2.2,2.25]\n",
    "gammas = [0.712,0.723,0.725,0.727,0.729]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas }\n",
    "grid_search = GridSearchCV(SVC(kernel=\"rbf\",class_weight='balanced'), param_grid,n_jobs=6, cv=4, scoring=\"f1_weighted\")\n",
    "grid_search.fit(X_tfidf, datadev['class'])\n",
    "grid_search.best_params_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
