# -*- coding: utf-8 -*-
"""Homework2_definitivo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1joNdyqTLTrZjSk4ltJS9i8zwjYi8Y99P

**Install requirements**
"""

!pip3 install 'torch==1.3.1'
!pip3 install 'torchvision==0.5.0'
!pip3 install 'Pillow-SIMD'
!pip3 install 'tqdm'

from google.colab import drive
drive.mount('/content/drive')

"""**Import libraries**"""

import os
import logging

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Subset, DataLoader
from torch.backends import cudnn

import torchvision
from torchvision import transforms
from torchvision.models import alexnet

from PIL import Image
from tqdm import tqdm

import matplotlib.pyplot as plt
import matplotlib

import numpy as np

!nvidia-smi

"""**Set Arguments**"""

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

NUM_CLASSES = 101 # 101 + 1: There is am extra Background class that should be removed 

BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing
                     # the batch size, learning rate should change by the same factor to have comparable results

LR = 1e-3            # The initial Learning Rate
MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD
WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default

NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)
STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)
GAMMA = 0.1          # Multiplicative factor for learning rate step-down

LOG_FREQUENCY = 10

"""**Define Data Preprocessing**"""

# Define transforms for training phase
train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256
                                      transforms.CenterCrop(224),  # Crops a central square patch of the image
                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!
                                                                   # Remember this when applying different transformations, otherwise you get an error
                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor
                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizes tensor with mean and standard deviation
])
# Define transforms for the evaluation phase
eval_transform = transforms.Compose([transforms.Resize(256),
                                      transforms.CenterCrop(224),
                                      transforms.ToTensor(),
                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    
])

"""**Prepare Dataset**"""

from torchvision.datasets import VisionDataset

from PIL import Image
import random

def pil_loader(path):
    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)
    with open(path, 'rb') as f:
        img = Image.open(f)
        return img.convert('RGB')

    

    
class Caltech(VisionDataset):

    
    
    def __init__(self, root, split='train', transform=None, target_transform=None):
        super(Caltech, self).__init__(root, transform=transform, target_transform=target_transform)
        
        self.data = {}
        self.labels = []
        self.split = split # This defines the split you are going to use
                           # (split files are called 'train.txt' and 'test.txt')
        f = open(root.split("/")[0]+"/"+split+".txt","r")
        rl = f.readlines()
        for l in rl:
            l = l.replace("\n","")
            if l.startswith("BACKGROUND"):
                continue
            else:
                img = pil_loader(root+"/"+l)
                label = l.split("/")[0]
                
                self.data[len(self.data)] = [img,self.__map__(label)]
        f.close()
        '''
        - Here you should implement the logic for reading the splits files and accessing elements
        - If the RAM size allows it, it is faster to store all data in memory
        - PyTorch Dataset classes use indexes to read elements
        - You should provide a way for the __getitem__ method to access the image-label pair
          through the index
        - Labels should start from 0, so for Caltech you will have lables 0...100 (excluding the background class) 
        '''
        
    def __map__(self,label):
        if label in self.labels:
            return self.labels.index(label)
        else:
            self.labels.append(label)
            return self.labels.index(label)
    
    def __getitem__(self, index):
        '''
        __getitem__ should access an element through its index
        Args:
            index (int): Index

        Returns:
            tuple: (sample, target) where target is class_index of the target class.
        '''
        try:
            image, label = self.data[index] # Provide a way to access image and label via index
                           # Image should be a PIL Image
                           # label can be int
        except IndexError:
            parent = ""
            return "error at index: "+index,label
            

        # Applies preprocessing when accessing the image
        if self.transform is not None and type(image) is not torch.Tensor:
            image = self.transform(image)

        return image, label

    def __len__(self):
        '''
        The __len__ method returns the length of the dataset
        It is mandatory, as this is used by several other components
        '''
        length = len(self.data) # Provide a way to get the length (number of elements) of the dataset
        return length

    def __indexes_split__(self,train_perc = 0.5):
        if(train_perc == 1):
            indexes = self.data.keys()
            labels = list( label for [img,label] in self.data.values())
            return list(zip(indexes,labels)),[]
        dataview = {}
        for key, val in self.data.items(): #{index,[img,label]}
            dataview[val[1]] = dataview.get(val[1],[]) + [key] 
        train_indexes = []
        val_indexes = []
        last_train = True
        for label in dataview.keys():
            tmp = dataview[label]
            split_index = int(len(tmp)*train_perc)
            if len(tmp) % 2 != 0:
                if last_train:
                    split_index += 1
                last_train = not last_train
            random.shuffle(tmp)
            train_indexes.extend(tmp[:split_index])
            val_indexes.extend(tmp[-(len(tmp)-split_index):])
        return train_indexes,val_indexes

    def __get_distribution__(self):
        dataview = {}
        for key, val in self.data.items(): #{index,[img,label]}
            label = self.labels[val[1]]
            dataview[label] = dataview.get(label,0) + 1 
        return dataview.keys(),dataview.values()


    def __augment__(self, aug_transform):
        augmented_data = {}
        it = 0
        for img,label in self.data.values():
            augmented_data[it] = [img,label]
            it += 1
            augmented_data[len(self.data)+it] = [aug_transform(img),label]
        self.data = augmented_data

# Clone github repository with data
if not os.path.isdir('./Caltech101'):
  !git clone https://github.com/MachineLearning2020/Homework2-Caltech101.git
  !mv 'Homework2-Caltech101' 'Caltech101'

DATA_DIR = 'Caltech101/101_ObjectCategories'
#from Caltech101.caltech_dataset import Caltech

# Prepare Pytorch train/test Datasets
train_dataset = Caltech(DATA_DIR, split='train',  transform=train_transform)
labels, counts = train_dataset.__get_distribution__()
print(labels)
plt.figure(figsize=(18, 15))
plt.bar(labels, counts,)
plt.xticks(rotation='vertical')
plt.show()


test_dataset = Caltech(DATA_DIR, split='test', transform=eval_transform)
labels, counts = test_dataset.__get_distribution__()
plt.figure(figsize=(18, 15))
plt.bar(labels, counts,)
plt.xticks(rotation='vertical')
plt.show()

#train_indexes = # split the indices for your train split
#val_indexes = # split the indices for your val split
print(train_dataset.__len__())

train_indexes,val_indexes = train_dataset.__indexes_split__(0.5)

val_dataset = Subset(train_dataset, train_indexes)
train_dataset = Subset(train_dataset, val_indexes)

# Check dataset sizes
print('Train Dataset: {}'.format(len(train_dataset)))
print('Valid Dataset: {}'.format(len(val_dataset)))
print('Test Dataset: {}'.format(len(test_dataset)))

train_dist = np.zeros(101)

for elem in train_dataset:
  train_dist[elem[1]] += 1

val_dist = np.zeros(101)

for elem in val_dataset:
  val_dist[elem[1]] += 1

plt.figure(figsize=(18, 15))
plt.bar(labels, train_dist,)
plt.xticks(rotation='vertical')
plt.show()

plt.figure(figsize=(18, 15))
plt.bar(labels, val_dist,)
plt.xticks(rotation='vertical')
plt.show()

"""**Prepare Dataloaders**"""

# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)
val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

def imshow(img):
    plt.figure(figsize=(15, 12))
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))

    plt.show()


# get some random training images
dataiter = iter(train_dataloader)
images, labels = dataiter.next()

# show images
imshow(torchvision.utils.make_grid(images))

"""**Prepare Network**"""

net = alexnet() # Loading AlexNet model

# AlexNet has 1000 output neurons, corresponding to the 1000 ImageNet's classes
# We need 101 outputs for Caltech-101
net.classifier[6] = nn.Linear(4096, NUM_CLASSES) # nn.Linear in pytorch is a fully connected layer
                                                 # The convolutional layer is nn.Conv2d

# We just changed the last layer of AlexNet with a new fully connected layer with 101 outputs
# It is strongly suggested to study torchvision.models.alexnet source code

"""**Prepare Training**"""

# Define loss function
criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy

# Choose parameters to optimize
# To access a different set of parameters, you have to access submodules of AlexNet
# (nn.Module objects, like AlexNet, implement the Composite Pattern)
# e.g.: parameters of the fully connected layers: net.classifier.parameters()
# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) 
parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet

# Define optimizer
# An optimizer updates the weights based on loss
# We use SGD with momentum
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)

# Define scheduler
# A scheduler dynamically changes learning rate
# The most common schedule is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)

"""**Train & Validation**"""

# By default, everything is loaded to cpu

def train(net,train_dataloader,val_dataloader,checkpoint_path):
  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda
  n_epochs_stop = 5
  min_val_loss = np.Inf
  epochs_no_improve = 0
  cudnn.benchmark # Calling this optimizes runtime
  train_loss , train_accuracy = [], []
  val_loss , val_accuracy = [], []

  current_step = 0
  # Start iterating over the epochs
  for epoch in range(NUM_EPOCHS):
    val_running_loss = 0.0
    val_running_correct = 0
    train_running_loss = 0.0
    train_running_correct = 0
    train_NUM_BATCHES = 0
    val_NUM_BATCHES = 0
    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr()))
    net.train(True)
    # Iterate over the dataset
    for images, labels in train_dataloader:  ##---------------------------------  TRAINING
      train_NUM_BATCHES += 1
      # Bring data over the device of choice
      images = images.to(DEVICE)
      labels = labels.to(DEVICE)

      net.train() # Sets module in training mode

      # PyTorch, by default, accumulates gradients after each backward pass
      # We need to manually set the gradients to zero before starting a new iteration
      optimizer.zero_grad() # Zero-ing the gradients

      # Forward pass to the network
      outputs = net(images)

      # Compute loss based on output and ground truth
      loss = criterion(outputs, labels)

      train_running_loss += loss.item()
      _, preds = torch.max(outputs.data, 1)
      train_running_correct += torch.sum(preds == labels.data).data.item()
      # Log loss
      if current_step % LOG_FREQUENCY == 0:
        print('Step {}, Loss {}'.format(current_step, loss.item()))

      # Compute gradients for each layer and update weights
      loss.backward()  # backward pass: computes gradients
      optimizer.step() # update weights based on accumulated gradients
      

      current_step += 1


    net.train(False) # Set Network to evaluation mode
    for val_images, val_labels in tqdm(val_dataloader): ##----------------------  VALIDATION
      val_NUM_BATCHES += 1
      val_images = val_images.to(DEVICE)
      val_labels = val_labels.to(DEVICE)

      # Forward Pass
      outputs = net(val_images)
      loss = criterion(outputs, val_labels)
      # Get predictions
      _, preds = torch.max(outputs.data, 1)

      # Update Corrects
      val_running_correct += torch.sum(preds == val_labels.data).data.item()

      val_running_loss += loss.item()

    print()
    epoch_train_loss = train_running_loss/train_NUM_BATCHES
    epoch_train_accuracy = train_running_correct/float(len(train_dataset))
    print(f"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.2f}")
    epoch_val_loss = val_running_loss/val_NUM_BATCHES
    epoch_val_accuracy = val_running_correct / float(len(val_dataset))
    print(f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.2f}')
    train_loss.append(epoch_train_loss)
    train_accuracy.append(epoch_train_accuracy)
    val_loss.append(epoch_val_loss)
    val_accuracy.append(epoch_val_accuracy)
    # Step the scheduler
    scheduler.step() 
    print()
    print()


    # If the validation loss is at a minimum
    if epoch_val_loss < min_val_loss:
      # Save the model
      torch.save(net, checkpoint_path)
      epochs_no_improve = 0
      min_val_loss = epoch_val_loss
      
    else:
      epochs_no_improve += 1
      # Check early stopping condition
      if epochs_no_improve == n_epochs_stop:
        print('Early stopping!')
        
        # Load in the best model
        net = torch.load(checkpoint_path)
  net = torch.load(checkpoint_path)
  return train_loss,train_accuracy,val_loss,val_accuracy,net

def print_acc_loss(train_loss,train_accuracy,val_loss,val_accuracy,title):
  print(list(zip(train_loss,train_accuracy)),list(zip(val_loss,val_accuracy)))
  # accuracy plots
  plt.figure(figsize=(10, 7))
  plt.plot(train_accuracy, color='green', label='train accuracy')
  plt.plot(val_accuracy, color='blue', label='validataion accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('Accuracy')
  plt.legend()
  plt.savefig(title+'_accuracy.png')
  plt.show()

  # loss plots
  plt.figure(figsize=(10, 7))
  plt.plot(train_loss, color='orange', label='train loss')
  plt.plot(val_loss, color='red', label='validataion loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()
  plt.savefig(title+'_loss.png')
  plt.show()

"""**Test**"""

def test(net, test_dataloader):
  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda
  net.train(False) # Set Network to evaluation mode

  running_corrects = 0
  for images, labels in tqdm(test_dataloader):
    images = images.to(DEVICE)
    labels = labels.to(DEVICE)

    # Forward Pass
    outputs = net(images)

    # Get predictions
    _, preds = torch.max(outputs.data, 1)

    # Update Corrects
    running_corrects += torch.sum(preds == labels.data).data.item()

  # Calculate Accuracy
  accuracy = running_corrects / float(len(test_dataset))

  print('Test Accuracy: {}'.format(accuracy))

tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet")
print_acc_loss(tl,ta,vl,va,"alexnet")
test(net,test_dataloader)

"""Hyperparameters experiments"""

LR = 1e-2
net = alexnet()
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet LR 1e-2")
print_acc_loss(tl,ta,vl,va,"alexnet LR 1e-2")
test(net,test_dataloader)

LR = 1e-1
net = alexnet()
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet LR 1e-2")
print_acc_loss(tl,ta,vl,va,"alexnet LR 1e-2")
test(net,test_dataloader)

STEP_SIZE = 15
LR = 1e-2
net = alexnet()
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet LR 1e-1")
print_acc_loss(tl,ta,vl,va,"alexnet LR 1e-1")
test(net,test_dataloader)

STEP_SIZE = 5
LR = 1e-2
net = alexnet()
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet LR 1e-1")
print_acc_loss(tl,ta,vl,va,"alexnet LR 1e-1")
test(net,test_dataloader)

STEP_SIZE = 10
LR = 1e-2
net = alexnet()
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet LR 1e-1")
print_acc_loss(tl,ta,vl,va,"alexnet LR 1e-1")
test(net,test_dataloader)

STEP_SIZE = 20
LR = 1e-2
WEIGHT_DECAY = 5e-4 
net = alexnet()
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet wd4")
print_acc_loss(tl,ta,vl,va,"alexnet LR 1e-1")
test(net,test_dataloader)

STEP_SIZE = 20
LR = 1e-2
WEIGHT_DECAY = 5e-6
net = alexnet()
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet wd4")
print_acc_loss(tl,ta,vl,va,"alexnet LR 1e-1")
test(net,test_dataloader)

STEP_SIZE = 20
LR = 1e-2
WEIGHT_DECAY = 5e-5
GAMMA = 0.2
net = alexnet()
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet wd4")
print_acc_loss(tl,ta,vl,va,"alexnet LR 1e-1")
test(net,test_dataloader)

STEP_SIZE = 20
LR = 1e-2
WEIGHT_DECAY = 5e-5
GAMMA = 0.3
net = alexnet()
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet wd4")
print_acc_loss(tl,ta,vl,va,"alexnet LR 1e-1")
test(net,test_dataloader)

STEP_SIZE = 20
LR = 1e-2
WEIGHT_DECAY = 5e-5
GAMMA = 0.5
net = alexnet()
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet wd4")
print_acc_loss(tl,ta,vl,va,"alexnet LR 1e-1")
test(net,test_dataloader)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

NUM_CLASSES = 101 # 101 + 1: There is am extra Background class that should be removed 

BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing
                     # the batch size, learning rate should change by the same factor to have comparable results

LR = 1e-2            # The initial Learning Rate
MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD
WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default

NUM_EPOCHS = 50      # Total number of training epochs (iterations over dataset)
STEP_SIZE = 10       # How many epochs before decreasing learning rate (if using a step-down policy)
GAMMA = 0.1          # Multiplicative factor for learning rate step-down

net = alexnet() # Loading AlexNet model
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
# Choose parameters to optimize
# To access a different set of parameters, you have to access submodules of AlexNet
# (nn.Module objects, like AlexNet, implement the Composite Pattern)
# e.g.: parameters of the fully connected layers: net.classifier.parameters()
# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) 
parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)


tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet_hp_test") 
print_acc_loss(tl,ta,vl,va,"alexnet_hp_test")
test(net,test_dataloader)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

NUM_CLASSES = 101 # 101 + 1: There is am extra Background class that should be removed 

BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing
                     # the batch size, learning rate should change by the same factor to have comparable results

LR = 1e-2            # The initial Learning Rate
MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD
WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default

NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)
STEP_SIZE = 10       # How many epochs before decreasing learning rate (if using a step-down policy)
GAMMA = 0.1          # Multiplicative factor for learning rate step-down

optimizer = optim.Adam(net.parameters(), lr=LR)
net = alexnet() # Loading AlexNet model
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
# Choose parameters to optimize
# To access a different set of parameters, you have to access submodules of AlexNet
# (nn.Module objects, like AlexNet, implement the Composite Pattern)
# e.g.: parameters of the fully connected layers: net.classifier.parameters()
# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) 
parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)

tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="alexnet_hp_test2")
print_acc_loss(tl,ta,vl,va,"alexnet_hp_test2")
test(net,test_dataloader)

"""*Transfer learning*"""

BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing
                     # the batch size, learning rate should change by the same factor to have comparable results

LR = 1e-3            # The initial Learning Rate
MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD
WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default

NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)
STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)
GAMMA = 0.1          # Multiplicative factor for learning rate step-down


preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
DATA_DIR = 'Caltech101/101_ObjectCategories'

train_dataset = Caltech(DATA_DIR, split='train',  transform=preprocess)
test_dataset = Caltech(DATA_DIR, split='test', transform=preprocess)
train_indexes,val_indexes = train_dataset.__indexes_split__(0.5)

val_dataset = Subset(train_dataset, train_indexes)
train_dataset = Subset(train_dataset, val_indexes)

# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)
val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

net = alexnet(pretrained = True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet pretrained")
print_acc_loss(tl,ta,vl,va,"alexnet pretrained")
test(net,test_dataloader)

LR = 1e-1
net = alexnet(pretrained = True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet pretrained")
print_acc_loss(tl,ta,vl,va,"alexnet pretrained")
test(net,test_dataloader)

LR = 1e-2
net = alexnet(pretrained = True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet pretrained")
print_acc_loss(tl,ta,vl,va,"alexnet pretrained")
test(net,test_dataloader)

LR = 1e-3
net = alexnet(pretrained = True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet pretrained")
print_acc_loss(tl,ta,vl,va,"alexnet pretrained")
test(net,test_dataloader)

LR = 1e-2
STEP_SIZE = 5
net = alexnet(pretrained = True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet pretrained")
print_acc_loss(tl,ta,vl,va,"alexnet pretrained")
test(net,test_dataloader)

LR = 1e-2
STEP_SIZE = 15
net = alexnet(pretrained = True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet pretrained")
print_acc_loss(tl,ta,vl,va,"alexnet pretrained")
test(net,test_dataloader)

LR = 1e-2
STEP_SIZE = 20
WEIGHT_DECAY = 5e-4
net = alexnet(pretrained = True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet pretrained")
print_acc_loss(tl,ta,vl,va,"alexnet pretrained")
test(net,test_dataloader)

LR = 1e-2
STEP_SIZE = 20
WEIGHT_DECAY = 5e-6
net = alexnet(pretrained = True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet pretrained")
print_acc_loss(tl,ta,vl,va,"alexnet pretrained")
test(net,test_dataloader)

LR = 1e-2
STEP_SIZE = 20
WEIGHT_DECAY = 5e-5
GAMMA = 0.3
net = alexnet(pretrained = True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet pretrained")
print_acc_loss(tl,ta,vl,va,"alexnet pretrained")
test(net,test_dataloader)

LR = 1e-2
STEP_SIZE = 20
WEIGHT_DECAY = 5e-4
GAMMA = 0.5
net = alexnet(pretrained = True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet pretrained")
print_acc_loss(tl,ta,vl,va,"alexnet pretrained")
test(net,test_dataloader)

"""Freeze layers"""

LR = 1e-2
STEP_SIZE = 20
WEIGHT_DECAY = 5e-5
GAMMA = 0.1
net = alexnet(pretrained = True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.classifier.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet pretrained freeze")
print_acc_loss(tl,ta,vl,va,"alexnet pretrained freeze")
test(net,test_dataloader)

LR = 1e-2
STEP_SIZE = 20
WEIGHT_DECAY = 5e-5
GAMMA = 0.1
net = alexnet(pretrained = True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.features.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet pretrained freeze")
print_acc_loss(tl,ta,vl,va,"alexnet pretrained freeze")
test(net,test_dataloader)

"""Data augmentation"""

BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing
                     # the batch size, learning rate should change by the same factor to have comparable results

LR = 1e-2            # The initial Learning Rate
MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD
WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default

NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)
STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)
GAMMA = 0.1          # Multiplicative factor for learning rate step-down


preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

train_data_augmentation = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(40),
    transforms.RandomVerticalFlip(p=0.5),
    transforms.RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=3, fill=0),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


DATA_DIR = 'Caltech101/101_ObjectCategories'

train_dataset = Caltech(DATA_DIR, split='train',  transform=train_data_augmentation)
test_dataset = Caltech(DATA_DIR, split='test', transform=preprocess)
train_indexes,val_indexes = train_dataset.__indexes_split__(0.5)

val_dataset = Subset(train_dataset, train_indexes)
train_dataset = Subset(train_dataset, val_indexes)

# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)
val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

net = alexnet(pretrained = True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.classifier.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet pretrained freeze")
print_acc_loss(tl,ta,vl,va,"alexnet pretrained freeze")
test(net,test_dataloader)

BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing
                     # the batch size, learning rate should change by the same factor to have comparable results

LR = 1e-2            # The initial Learning Rate
MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD
WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default

NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)
STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)
GAMMA = 0.1          # Multiplicative factor for learning rate step-down


preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


train_data_augmentation = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(80),
    transforms.RandomVerticalFlip(p=0.5),
    transforms.ColorJitter(hue=.05, saturation=.05),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


DATA_DIR = 'Caltech101/101_ObjectCategories'

train_dataset = Caltech(DATA_DIR, split='train',  transform=train_data_augmentation)
test_dataset = Caltech(DATA_DIR, split='test', transform=preprocess)
train_indexes,val_indexes = train_dataset.__indexes_split__(0.5)

val_dataset = Subset(train_dataset, train_indexes)
train_dataset = Subset(train_dataset, val_indexes)

# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)
val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

net = alexnet(pretrained = True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.classifier.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet pretrained freeze")
print_acc_loss(tl,ta,vl,va,"alexnet pretrained freeze")
test(net,test_dataloader)

BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing
                     # the batch size, learning rate should change by the same factor to have comparable results

LR = 1e-2            # The initial Learning Rate
MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD
WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default

NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)
STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)
GAMMA = 0.1          # Multiplicative factor for learning rate step-down


preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


train_data_augmentation = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    torchvision.transforms.Grayscale(num_output_channels=3),
    transforms.ColorJitter(hue=.05, saturation=.05),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

DATA_DIR = 'Caltech101/101_ObjectCategories'

train_dataset = Caltech(DATA_DIR, split='train',  transform=train_data_augmentation)
test_dataset = Caltech(DATA_DIR, split='test', transform=preprocess)
train_indexes,val_indexes = train_dataset.__indexes_split__(0.5)

val_dataset = Subset(train_dataset, train_indexes)
train_dataset = Subset(train_dataset, val_indexes)

# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)
val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

net = alexnet(pretrained = True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.classifier.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet pretrained freeze")
print_acc_loss(tl,ta,vl,va,"alexnet pretrained freeze")
test(net,test_dataloader)

from torchvision import models
dir(models)

def initialize_model(model_name,num_ftrs,num_classes,use_pretrained=True):
    # Initialize these variables which will be set in this if statement. Each of these
    #   variables is model specific.
    model_ft = None
    input_size = 0
    use_linear = True

    if model_name == "resnet":
        """ Resnet18
        """
        model_ft = models.resnet18(pretrained=use_pretrained)
        
    elif model_name == "resnet50":
        """ Resnet50
        """
        model = models.resnet50(pretrained=use_pretrained)
        num_ftrs = model.fc.in_features
        model.fc = nn.Linear(num_ftrs, num_classes)
        for name, child in model.named_children():
            for name2, params in child.named_parameters():
                params.requires_grad = True
        return model,224

    elif model_name == "alexnet":
        """ Alexnet
        """
        model_ft = models.alexnet(pretrained=use_pretrained)

    elif model_name == "vgg":
        """ VGG11_bn
        """
        model_ft = models.vgg11_bn(pretrained=use_pretrained)
        
    elif model_name == "vgg16":
        """ VGG16
        """
        model_ft = models.vgg16(pretrained=use_pretrained)
        use_linear = False

    elif model_name == "squeezenet":
        """ Squeezenet
        """
        model_ft = models.squeezenet1_0(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))
        model_ft.num_classes = num_classes
        input_size = 224
        use_linear = False

    elif model_name == "densenet":
        """ Densenet
        """
        model_ft = models.densenet121(pretrained=use_pretrained)
        
        

    elif model_name == "inception":
        """ Inception v3
        Be careful, expects (299,299) sized images and has auxiliary output
        """
        model_ft = models.inception_v3(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        # Handle the auxilary net
        num_ftrs = model_ft.AuxLogits.fc.in_features
        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)


    else:
        print("Invalid model name, exiting...")
        exit()
    
    if use_pretrained:
        for param in model_ft.classifier[6].parameters():
            param.requires_grad = True
    
    if use_linear:
        model_ft.classifier = nn.Linear(num_ftrs, num_classes)
        input_size = 224

    return model_ft, input_size

"""Resnet50"""

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

NUM_CLASSES = 101 # 101 + 1: There is am extra Background class that should be removed 

BATCH_SIZE = 16   # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing
                     # the batch size, learning rate should change by the same factor to have comparable results

LR = 1e-2            # The initial Learning Rate
MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD
WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default

NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)
STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)
GAMMA = 0.1          # Multiplicative factor for learning rate step-down
LOG_FREQUENCY = 10


preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


DATA_DIR = 'Caltech101/101_ObjectCategories'

train_dataset = Caltech(DATA_DIR, split='train',  transform=preprocess)
test_dataset = Caltech(DATA_DIR, split='test', transform=preprocess)
train_indexes,val_indexes = train_dataset.__indexes_split__(0.5)

val_dataset = Subset(train_dataset, train_indexes)
train_dataset = Subset(train_dataset, val_indexes)

# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)
val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

net, input_size = initialize_model("resnet50",4096,NUM_CLASSES,use_pretrained=True)

print(net)

parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="alexnet_transfer_learning")
print_acc_loss(tl,ta,vl,va,"alexnet_transfer_learning")
test(net,test_dataloader)

"""VGG16"""

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

NUM_CLASSES = 101 # 101 + 1: There is am extra Background class that should be removed 

BATCH_SIZE = 8     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing
                     # the batch size, learning rate should change by the same factor to have comparable results

LR = 1e-3            # The initial Learning Rate
MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD
WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default

NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)
STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)
GAMMA = 0.1          # Multiplicative factor for learning rate step-down
LOG_FREQUENCY = 10


preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


DATA_DIR = 'Caltech101/101_ObjectCategories'

train_dataset = Caltech(DATA_DIR, split='train',  transform=preprocess)
test_dataset = Caltech(DATA_DIR, split='test', transform=preprocess)
train_indexes,val_indexes = train_dataset.__indexes_split__(0.5)

val_dataset = Subset(train_dataset, train_indexes)
train_dataset = Subset(train_dataset, val_indexes)

# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)
val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

net, input_size = initialize_model("vgg16",4096,NUM_CLASSES,use_pretrained=True)

print(net)

parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="alexnet_transfer_learning")
print_acc_loss(tl,ta,vl,va,"alexnet_transfer_learning")
test(net,test_dataloader)



"""Further personal experiments, such as more epochs, dataset augmented."""

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

NUM_CLASSES = 101 # 101 + 1: There is am extra Background class that should be removed 

BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing
                     # the batch size, learning rate should change by the same factor to have comparable results

LR = 1e-2            # The initial Learning Rate
MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD
WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default

NUM_EPOCHS = 50      # Total number of training epochs (iterations over dataset)
STEP_SIZE = 15       # How many epochs before decreasing learning rate (if using a step-down policy)
GAMMA = 0.1          # Multiplicative factor for learning rate step-down
LOG_FREQUENCY = 10
#default alexnet transformation on ImageNet
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


DATA_DIR = 'Caltech101/101_ObjectCategories'

train_dataset = Caltech(DATA_DIR, split='train',  transform=preprocess)
test_dataset = Caltech(DATA_DIR, split='test', transform=preprocess)
train_indexes,val_indexes = train_dataset.__indexes_split__(0.5)

val_dataset = Subset(train_dataset, train_indexes)
train_dataset = Subset(train_dataset, val_indexes)

# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)
val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

net = alexnet(pretrained = True) # Loading AlexNet model


net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
# Choose parameters to optimize
# To access a different set of parameters, you have to access submodules of AlexNet
# (nn.Module objects, like AlexNet, implement the Composite Pattern)
# e.g.: parameters of the fully connected layers: net.classifier.parameters()
# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) 
parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="alexnet_transfer_learning")
print_acc_loss(tl,ta,vl,va,"alexnet_transfer_learning")
test(net,test_dataloader)

"""*Transfer learning + FC-layers freeze*"""

NUM_CLASSES = 101 # 101 + 1: There is am extra Background class that should be removed 

BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing
                     # the batch size, learning rate should change by the same factor to have comparable results

LR = 1e-2            # The initial Learning Rate
MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD
WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default

NUM_EPOCHS = 50      # Total number of training epochs (iterations over dataset)
STEP_SIZE = 15       # How many epochs before decreasing learning rate (if using a step-down policy)
GAMMA = 0.1          # Multiplicative factor for learning rate step-down
LOG_FREQUENCY = 10

net = alexnet(pretrained = True)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
# Freeze model weights
for param in net.parameters():
    param.requires_grad = False

net.classifier = nn.Sequential(
  nn.Dropout(p=0.5, inplace=False),
  nn.Linear(in_features=9216, out_features=4096, bias=True),
  nn.ReLU(inplace=True),
  nn.Dropout(p=0.5, inplace=False),
  nn.Linear(in_features=4096, out_features=4096, bias=True),
  nn.ReLU(inplace=True),
  nn.Linear(in_features=4096, out_features=NUM_CLASSES, bias=True)
)

total_params = sum(p.numel() for p in net.parameters())
print(f'{total_params:,} total parameters.')
total_trainable_params = sum(
    p.numel() for p in net.parameters() if p.requires_grad)
print(f'{total_trainable_params:,} training parameters.')
criterion = nn.CrossEntropyLoss() 
parameters_to_optimize = net.parameters() 
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="./alexnet pretrained")
print_acc_loss(tl,ta,vl,va,"alexnet pretrained")
test(net,test_dataloader)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

NUM_CLASSES = 101 # 101 + 1: There is am extra Background class that should be removed 

BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing
                     # the batch size, learning rate should change by the same factor to have comparable results

LR = 1e-2            # The initial Learning Rate
MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD
WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default

NUM_EPOCHS = 50      # Total number of training epochs (iterations over dataset)
STEP_SIZE = 15       # How many epochs before decreasing learning rate (if using a step-down policy)
GAMMA = 0.1          # Multiplicative factor for learning rate step-down
LOG_FREQUENCY = 10
#default alexnet transformation on ImageNet
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


DATA_DIR = 'Caltech101/101_ObjectCategories'

train_dataset = Caltech(DATA_DIR, split='train',  transform=preprocess)
test_dataset = Caltech(DATA_DIR, split='test', transform=preprocess)
train_indexes,val_indexes = train_dataset.__indexes_split__(0.5)

val_dataset = Subset(train_dataset, train_indexes)
train_dataset = Subset(train_dataset, val_indexes)

# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)
val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

net = alexnet(pretrained = True) # Loading AlexNet model

# Freeze model weights
for param in net.parameters():
    param.requires_grad = False

net.classifier = nn.Sequential(
  nn.Dropout(p=0.5, inplace=False),
  nn.Linear(in_features=9216, out_features=4096, bias=True),
  nn.ReLU(inplace=True),
  nn.Dropout(p=0.5, inplace=False),
  nn.Linear(in_features=4096, out_features=4096, bias=True),
  nn.ReLU(inplace=True),
  nn.Linear(in_features=4096, out_features=NUM_CLASSES, bias=True)
)

total_params = sum(p.numel() for p in net.parameters())
print(f'{total_params:,} total parameters.')
total_trainable_params = sum(
    p.numel() for p in net.parameters() if p.requires_grad)
print(f'{total_trainable_params:,} training parameters.')

criterion = nn.CrossEntropyLoss() 
# Choose parameters to optimize
# To access a different set of parameters, you have to access submodules of AlexNet
# (nn.Module objects, like AlexNet, implement the Composite Pattern)
# e.g.: parameters of the fully connected layers: net.classifier.parameters()
# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) 
parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="alexnet_tl_fc_freeze")
print_acc_loss(tl,ta,vl,va,"alexnet_tl_fc_freeze")
test(net,test_dataloader)

"""*Transfer learning + Conv-layers freeze*"""

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

NUM_CLASSES = 101 # 101 + 1: There is am extra Background class that should be removed 

BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing
                     # the batch size, learning rate should change by the same factor to have comparable results

LR = 1e-2            # The initial Learning Rate
MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD
WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default

NUM_EPOCHS = 50      # Total number of training epochs (iterations over dataset)
STEP_SIZE = 15       # How many epochs before decreasing learning rate (if using a step-down policy)
GAMMA = 0.1          # Multiplicative factor for learning rate step-down
LOG_FREQUENCY = 10
#default alexnet transformation on ImageNet
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


DATA_DIR = 'Caltech101/101_ObjectCategories'

train_dataset = Caltech(DATA_DIR, split='train',  transform=preprocess)
test_dataset = Caltech(DATA_DIR, split='test', transform=preprocess)
train_indexes,val_indexes = train_dataset.__indexes_split__(0.5)

val_dataset = Subset(train_dataset, train_indexes)
train_dataset = Subset(train_dataset, val_indexes)

# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)
val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

net = alexnet(pretrained = True) # Loading AlexNet model

net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
# Freeze model weights
for param in net.parameters():
    param.requires_grad = False

# Unfreeze conv layer weigths
net.features = nn.Sequential(
  nn.Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)),
  nn.ReLU(inplace=True),
  nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),
  nn.Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)),
  nn.ReLU(inplace=True),
  nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),
  nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
  nn.ReLU(inplace=True),
  nn.Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
  nn.ReLU(inplace=True),
  nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
  nn.ReLU(inplace=True),
  nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
)

total_params = sum(p.numel() for p in net.parameters())
print(f'{total_params:,} total parameters.')
total_trainable_params = sum(
    p.numel() for p in net.parameters() if p.requires_grad)
print(f'{total_trainable_params:,} training parameters.')


criterion = nn.CrossEntropyLoss() 
# Choose parameters to optimize
# To access a different set of parameters, you have to access submodules of AlexNet
# (nn.Module objects, like AlexNet, implement the Composite Pattern)
# e.g.: parameters of the fully connected layers: net.classifier.parameters()
# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) 
parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="alexnet_tl_conv_freeze")
print_acc_loss(tl,ta,vl,va,"alexnet_tl_conv_freeze")
test(net,test_dataloader)

"""*Transfer learning + data augmentation*"""

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

NUM_CLASSES = 101 # 101 + 1: There is am extra Background class that should be removed 

BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing
                     # the batch size, learning rate should change by the same factor to have comparable results

LR = 1e-2            # The initial Learning Rate
MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD
WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default

NUM_EPOCHS = 50      # Total number of training epochs (iterations over dataset)
STEP_SIZE = 15       # How many epochs before decreasing learning rate (if using a step-down policy)
GAMMA = 0.1          # Multiplicative factor for learning rate step-down
LOG_FREQUENCY = 10
#default alexnet transformation on ImageNet
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

augment_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256
                                      transforms.CenterCrop(224),  # Crops a central square patch of the image
                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!
                                                                   # Remember this when applying different transformations, otherwise you get an error
                                      transforms.ColorJitter(hue=.05, saturation=.05),
                                      transforms.RandomHorizontalFlip(),
                                      transforms.RandomRotation(20, resample=Image.BILINEAR),
                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor
                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                                      
                                      #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizes tensor with mean and standard deviation
])


DATA_DIR = 'Caltech101/101_ObjectCategories'

train_dataset = Caltech(DATA_DIR, split='train',  transform=preprocess)
print("train dataset length: ",len(train_dataset))
train_dataset.__augment__(augment_transform)
print("augmented train dataset length: ",len(train_dataset))
test_dataset = Caltech(DATA_DIR, split='test', transform=preprocess)
train_indexes,val_indexes = train_dataset.__indexes_split__(0.5)

val_dataset = Subset(train_dataset, train_indexes)
train_dataset = Subset(train_dataset, val_indexes)

# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)
val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

net = alexnet(pretrained = True) # Loading AlexNet model


net.classifier[6] = nn.Linear(4096, NUM_CLASSES)
criterion = nn.CrossEntropyLoss() 
# Choose parameters to optimize
# To access a different set of parameters, you have to access submodules of AlexNet
# (nn.Module objects, like AlexNet, implement the Composite Pattern)
# e.g.: parameters of the fully connected layers: net.classifier.parameters()
# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) 
parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet
optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)

tl,ta,vl,va,net = train(net,train_dataloader=train_dataloader,val_dataloader=val_dataloader,checkpoint_path="alexnet_tf_and_dataaug")
print_acc_loss(tl,ta,vl,va,"alexnet_tf_and_dataaug")
test(net,test_dataloader)